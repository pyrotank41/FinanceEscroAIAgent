{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Synthetic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Fetch the page\n",
    "url = 'https://www.consumerfinance.gov/rules-policy/regulations/1024/17/'\n",
    "response = requests.get(url)\n",
    "html_content = response.text\n",
    "\n",
    "# Parse the HTML\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Extract all paragraphs\n",
    "paragraphs = soup.find_all('p')\n",
    "# Combine all text into a single string for easier processing\n",
    "full_text = '\\n'.join([p.get_text() for p in paragraphs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1250, chunk_overlap=20)\n",
    "\n",
    "chunks = text_splitter.split_text(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of chunks: \", len(chunks))\n",
    "print(\"Chunk 1: \", chunks[0])\n",
    "print(\"Chunk 2: \", chunks[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the chunks to a json file for future tracability\n",
    "import json\n",
    "with open('chunks.json', 'w') as f:\n",
    "    json.dump(chunks, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from utility.utils import get_openai_api_key\n",
    "\n",
    "OpenAI.api_key = get_openai_api_key()\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "# Role\n",
    "you are a worldclass llm training data generator. \n",
    "\n",
    "# Task\n",
    "You are generating q/a pairs for finetuning our own llm model.\n",
    "create a json of questions and answers. \n",
    "\n",
    "# Specifics\n",
    "Context relecance for the Q/A is really important, else our business will loose money on computation resources \n",
    "\n",
    "         \n",
    "# Example format:\n",
    "{ data: [\n",
    "    {\n",
    "        \"q\": <question>,\n",
    "        \"a\": <answer>\n",
    "    },\n",
    "    {\n",
    "        \"q\": <question>,\n",
    "        \"a\": <answer>\n",
    "    }, ...\n",
    "]}\n",
    "\n",
    "# Notes\n",
    "Make sure to generate JSON format for the Q/A pairs\n",
    "Generate 10 Q/A pairs\n",
    "          \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_qa_pairs(system_prompt, text):\n",
    "\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": text\n",
    "            }\n",
    "        ],\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        response_format={\"type\": \"json_object\"}\n",
    "    )\n",
    "\n",
    "    return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test out the generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets do a test run\n",
    "input = \"(a) General. This section sets out the requirements for an escrow account that a lender establishes in connection with a federally related mortgage loan. It sets limits for escrow accounts using calculations based on monthly payments and disbursements within a calendar year. If an escrow account involves biweekly or any other payment period, the requirements in this section shall be modified accordingly. A Public Guidance Document entitled “Biweekly Payments - Example” provides examples of biweekly accounting and a Public Guidance Document entitled “Annual Escrow Account Disclosure Statement - Example” provides examples of a 3-year accounting cycle that may be used in accordance with paragraph (c)(9) of this section. A Public Guidance Document entitled “Consumer Disclosure for Voluntary Escrow Account Payments” provides a model disclosure format that originators and servicers are encouraged, but not required, to provide to consumers when the originator or servicer anticipates a substantial increase in disbursements from the escrow account after the first year of the loan. The disclosures in that model format may be combined with or included in the Initial Escrow Account Statement required in § 1024.17(g).\"\n",
    "qa_pairs = generate_qa_pairs(SYSTEM_PROMPT, input)\n",
    "qa_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the chunk and and our generation of 10 q/a pairs data for each chunk, we will effectively have about 410 q/a pairs from the 41 chunks. This is a good start for the training data. We can always add more data later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chunks = {}\n",
    "qa_chunks[\"data\"] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: This code will take a long time to run (about 7 min) and will use up a lot of OpenAI credits\n",
    "# WARNING: This code will rewrite the existing qa_chunks.json file\n",
    "if False:\n",
    "    import json\n",
    "    count = 0\n",
    "    last_chunk__ending_index= 0\n",
    "    for chunk in chunks:\n",
    "        qa_pairs = generate_qa_pairs(SYSTEM_PROMPT, chunk)\n",
    "        if isinstance(qa_pairs, str):\n",
    "            qa_pairs = json.loads(qa_pairs)\n",
    "        qa_chunk = {\n",
    "                    \"id\": count,\n",
    "                    \"metadata\":{\n",
    "                        \"text\": chunk,\n",
    "                        \"length\": len(chunk),\n",
    "                        \"start_index\": last_chunk__ending_index,\n",
    "                        \"end_index\": last_chunk__ending_index + len(chunk)\n",
    "                    }, \n",
    "                    \"data\": qa_pairs.get('data')}\n",
    "        count+=1\n",
    "        last_chunk__ending_index += len(chunk)\n",
    "        qa_chunks.get('data').append(qa_chunk)\n",
    "        \n",
    "        # save qa_chunks to a json file so we dont loose the data\n",
    "        with open('qa_chunks.json', 'w') as f:\n",
    "            json.dump(qa_chunks, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Q/A pairs generated:  368\n",
      "Last chunk id:  40\n"
     ]
    }
   ],
   "source": [
    "# read the qa_chunks from the json file\n",
    "import json\n",
    "with open('generated_qa_data/qa_chunks.json') as f:\n",
    "    generated_qa_chunks = json.load(f)\n",
    "\n",
    "total_qa_generated = 0\n",
    "for chunk in generated_qa_chunks.get('data'):\n",
    "    total_qa_generated += len(chunk.get('data'))\n",
    "print(\"Total Q/A pairs generated: \", total_qa_generated)\n",
    "print(\"Last chunk id: \", generated_qa_chunks.get('data')[-1].get('id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we didnt get expected number of Q/A pairs, lets see what happened\n",
    "for chunk in qa_chunks.get('data'):\n",
    "    print(\"Chunk id: \", chunk.get('id'), \"Q/A pairs: \", len(chunk.get('data')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chunks.get('data')[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating additional data\n",
    "\n",
    "The generated q/a pairs are less than 400, so we will generate more data (we need atlease 1000 sample points).\n",
    "\n",
    "This time rather then using GPT-3.5 turbo we can use a smaller model as we are not trying to create Q/A from a piece of text, rather we are just generating a similar q/a pairs using the already generated q/a pair with more capable models.\n",
    "\n",
    "for this part we will use llama3-8b-instruct model for the generation of the q/a pairs.\n",
    "\n",
    "lets pull the model using ollama\n",
    "```ollama pull llama3```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama pull llama3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import ollama\n",
    "# since we are using smaller model (8b) and like openai's api, we dont have a guarentee of Json output, we will generate the similar output one at a time\n",
    "\n",
    "def generate_similar_question(question, additional_context=None):\n",
    "    \n",
    "    message = f'please reright the question in other words: {question}\\n note: only respond with the question, no answer needed. You can simplify the question as well if needed.'\n",
    "    if additional_context:\n",
    "        message = f'{additional_context} {message}'\n",
    "        \n",
    "    response = ollama.chat(\n",
    "        model='llama3',\n",
    "        messages=[\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': message\n",
    "            }\n",
    "        ],\n",
    "        \n",
    "        )\n",
    "    new_question = response['message']['content']\n",
    "    return new_question\n",
    "\n",
    "\n",
    "def generate_10_similar_questions(orignal_question, additional_context=None):\n",
    "\n",
    "    questions = []\n",
    "    print(f\"Generating similar questions like '{orignal_question}'\")\n",
    "    while True:\n",
    "        new_question = generate_similar_question(\n",
    "            orignal_question, additional_context)\n",
    "\n",
    "        # we see some wierd behavour from ollama api, when your request text is the same as the previous one, it may return the same text\n",
    "        # hence we will skip to save the same text\n",
    "        if new_question in questions:\n",
    "            continue\n",
    "        questions.append(new_question)\n",
    "        time.sleep(0.5)\n",
    "        print(\".\", end=\"\")\n",
    "        if len(questions) == 10:\n",
    "            break\n",
    "        \n",
    "    return questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_10_similar_questions(\"Why is the sky blue?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see above how the local llm is generation similar questions to the origninal, now we will use the same function to generate more data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_10_similar_questions(\n",
    "    \"What is the definition of Aggregate analysis in the context of escrow account analysis?\",\n",
    "    additional_context=\"remove the term escrow account from your resposne\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: This code will take a long time to run (about 60 min on macbook m2 pro 32 gb) and will use up a lot of local Compute Resource\n",
    "# now lets do the same for all the qenerated Q/A pairs, and save them to a new json file\n",
    "if False:\n",
    "    for chunk in generated_qa_chunks.get('data'):\n",
    "        for qa_pair in chunk.get('data'):\n",
    "            new_questions = generate_10_similar_questions(qa_pair.get('q'))\n",
    "            qa_pair['similar_questions'] = new_questions\n",
    "        \n",
    "        # save qa_chunks to a json file so we dont loose the data\n",
    "        with open('qa_chunks_with_similar_questions.json', 'w') as f:\n",
    "            json.dump(generated_qa_chunks, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_qa_chunks.get('data')[0].get('data')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have 3000+ q/a pairs after we pair the generated data with the GPT-3.5-turbo generated data. ideally it will be good to have similar answers generated as well but in the interest of time we will just use the questions generated by the llama3-8b-instruct model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"question\": \"What are the requirements for an escrow account established by a lender in connection with a federally related mortgage loan?\", \"answer\": \"The requirements for an escrow account set limits based on monthly payments and disbursements within a calendar year. If the escrow account involves a different payment period like biweekly, the requirements are modified accordingly.\", \"chunk_id\": 0, \"relevent_text\": \"(a) General. This section sets out the requirements for an escrow account that a lender establishes in connection with a federally related mortgage loan. It sets limits for escrow accounts using calculations based on monthly payments and disbursements within a calendar year. If an escrow account involves biweekly or any other payment period, the requirements in this section shall be modified accordingly. A Public Guidance Document entitled \\u201cBiweekly Payments - Example\\u201d provides examples of biweekly accounting and a Public Guidance Document entitled \\u201cAnnual Escrow Account Disclosure Statement - Example\\u201d provides examples of a 3-year accounting cycle that may be used in accordance with paragraph (c)(9) of this section. A Public Guidance Document entitled \\u201cConsumer Disclosure for Voluntary Escrow Account Payments\\u201d provides a model disclosure format that originators and servicers are encouraged, but not required, to provide to consumers when the originator or servicer anticipates a substantial increase in disbursements from the escrow account after the first year of the loan. The disclosures in that model format may be combined with or included in the Initial Escrow Account Statement required in \\u00a7\\u00a01024.17(g).\"}\n",
      "4048\n"
     ]
    }
   ],
   "source": [
    "# load the saved qa_chunks_with_similar_questions.json file\n",
    "import json\n",
    "with open('generated_qa_data/qa_chunks_with_similar_questions.json') as f:\n",
    "    generated_qa_chunks = json.load(f)\n",
    "\n",
    "### lets bring everything together, and generate the final dataset for fine-tuning the model.\n",
    "# we need a csv file with cloums: question, answer, chunk_id\n",
    "qa = []\n",
    "for chunk in generated_qa_chunks.get('data'):\n",
    "    for qa_pair in chunk.get('data'):\n",
    "        new_pair = {\n",
    "            \"question\": qa_pair.get('q'),\n",
    "            \"answer\": qa_pair.get('a'),\n",
    "            \"chunk_id\": chunk.get('id'),\n",
    "            \"relevent_text\": chunk.get('metadata').get('text'),\n",
    "        }\n",
    "        qa.append(new_pair)\n",
    "        for similar_question in qa_pair.get('similar_questions'):\n",
    "            new_pair = {\n",
    "                \"question\": similar_question,\n",
    "                \"answer\": qa_pair.get('a'),\n",
    "                \"chunk_id\": chunk.get('id'),\n",
    "                \"relevent_text\": chunk.get('metadata').get('text'),\n",
    "            }\n",
    "            qa.append(new_pair)\n",
    "        \n",
    "print(qa[0])\n",
    "print(len(qa))\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the pairs to a csv file\n",
    "# Save the data to a csv file.\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(qa)\n",
    "df.to_csv('qa_finetuning_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the data to make sure it was saved correctly\n",
    "df = pd.read_csv('generated_qa_data/qa_finetuning_dataset.csv')\n",
    "# adding an extra colum \"input\", and renaming the colum question to \"input\"and answer to \"output\"\n",
    "df[\"instruction\"] = df['question']\n",
    "df['output'] = df['answer']\n",
    "df = df.drop(columns=['question', 'answer', 'chunk_id'])\n",
    "# add an empty colum input \n",
    "df['input'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
