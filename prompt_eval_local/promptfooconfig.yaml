# This configuration compares LLM output of 2 prompts x 2 GPT models across 3 test cases.
# Learn more: https://promptfoo.dev/docs/configuration/guide
description: 'Simple Query Eval'
providers: 
  - id: ollama:llama3
  - id: ollama:escro_gemma:latest
  - id: ollama:gemma
prompts: [prompt.json]
tests: test*.yaml